{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78cc960f-20f4-4f70-9202-ec11e0d9c34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Read Streaming Data from Kafka#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "157e44ab-93c3-44f3-b785-4fa7e69e07a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Configure Kafka bootstrap servers\n",
    "kafka_bootstrap_servers = \"bore.pub:9093\"\n",
    "\n",
    "# Read Kafka data in streaming mode with additional timeout configurations\n",
    "streaming_df = (spark.readStream\n",
    "               .format(\"kafka\")\n",
    "               .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
    "               .option(\"subscribe\", \"music_streaming_events\")\n",
    "               .option(\"startingOffsets\", \"earliest\")\n",
    "               .option(\"kafka.metadata.fetch.timeout.ms\", \"30000\")  # Increase metadata fetch timeout to 30 seconds\n",
    "               .option(\"kafka.request.timeout.ms\", \"60000\")         # Increase request timeout to 60 seconds\n",
    "               .option(\"kafka.session.timeout.ms\", \"30000\")         # Increase session timeout to 30 seconds\n",
    "               .load())\n",
    "\n",
    "# Cast the Kafka message value to a string\n",
    "raw_streaming_df = streaming_df.select(col(\"value\").cast(\"STRING\").alias(\"message\"))\n",
    "\n",
    "# Write the raw stream to the console for debugging\n",
    "query_raw = (raw_streaming_df.writeStream\n",
    "             .format(\"console\")\n",
    "             .outputMode(\"append\")\n",
    "             .trigger(processingTime=\"10 seconds\")  # Process every 10 seconds\n",
    "             .start())\n",
    "\n",
    "# Wait for the query to terminate (for debugging)\n",
    "query_raw.awaitTermination(60)  # Wait up to 60 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8033fdde-cc1c-4985-9839-77455d8d5cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Parse the JSON Messages#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee23783a-6687-4cee-a842-c19741b3aa1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Define the schema for the JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"artist\", StringType(), True),\n",
    "    StructField(\"genre\", StringType(), True),\n",
    "    StructField(\"listen_timestamp\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"sales\", DoubleType(), True),\n",
    "    StructField(\"streams\", DoubleType(), True),\n",
    "    StructField(\"downloads\", DoubleType(), True),\n",
    "    StructField(\"radio_plays\", DoubleType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Parse the JSON data into structured columns\n",
    "parsed_streaming_df = raw_streaming_df.withColumn(\"data\", from_json(col(\"message\"), schema)).select(\"data.*\")\n",
    "\n",
    "# Filter out records with null user_id\n",
    "parsed_streaming_df = parsed_streaming_df.filter(col(\"user_id\").isNotNull())\n",
    "\n",
    "# Scale the streams column (multiply by 100 million)\n",
    "parsed_streaming_df = parsed_streaming_df.withColumn(\"streams\", col(\"streams\") * 1e8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0618b1b-c16f-46fa-a06e-a9dd08bd0af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Compute Average Streams per Genre#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2c2785-c6c6-42c5-a87b-3e23e824db76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, current_timestamp\n",
    "\n",
    "# Compute average streams per genre over a 1-minute tumbling window\n",
    "windowed_avg_streams = (parsed_streaming_df\n",
    "                        .groupBy(\n",
    "                            window(current_timestamp(), \"1 minute\"),\n",
    "                            \"genre\"\n",
    "                        )\n",
    "                        .agg({\"streams\": \"avg\"})\n",
    "                        .withColumnRenamed(\"avg(streams)\", \"avg_streams\"))\n",
    "\n",
    "# Write the results to an in-memory table for real-time querying\n",
    "query_avg = (windowed_avg_streams\n",
    "             .writeStream\n",
    "             .format(\"memory\")\n",
    "             .queryName(\"avg_streams_table\")\n",
    "             .outputMode(\"complete\")\n",
    "             .start())\n",
    "\n",
    "# For debugging, display the in-memory table (run this in a separate cell or loop)\n",
    "spark.sql(\"SELECT genre, avg_streams FROM avg_streams_table ORDER BY avg_streams DESC LIMIT 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b438c544-7868-4652-a1ed-5bec0cb616cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Stopping Query#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3c4b76-475b-4a56-9d80-46c028815a4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1719398306228774>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Stop the streaming query\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:372\u001B[0m, in \u001B[0;36mStreamingQuery.stop\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstop\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    350\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    351\u001B[0m \u001B[38;5;124;03m    Stop this streaming query.\u001B[39;00m\n",
       "\u001B[1;32m    352\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n",
       "\u001B[1;32m    371\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 372\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o573.stop.\n",
       ": java.util.concurrent.TimeoutException: Stream Execution thread for stream [id = ef5ae52f-dbf6-49b4-929b-9968ae3f0d1a, runId = e7b396bc-19f6-462a-863b-4168c2309ae6] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:716)\n",
       "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:409)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: The stream thread was last executing:\n",
       "\tat java.lang.Object.wait(Native Method)\n",
       "\tat java.lang.Thread.join(Thread.java:1265)\n",
       "\tat kafkashaded.org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:667)\n",
       "\tat kafkashaded.org.apache.kafka.clients.admin.Admin.close(Admin.java:154)\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.stopAdmin(KafkaOffsetReaderAdmin.scala:555)\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.close(KafkaOffsetReaderAdmin.scala:114)\n",
       "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.stop(KafkaMicroBatchStream.scala:382)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:631)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:629)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9439/1496207081.apply(Unknown Source)\n",
       "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:629)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$7(StreamExecution.scala:525)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9438/1527686176.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:514)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$8058/399919099.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat com.databricks.logging.UsageLogging$$Lambda$466/2118482375.apply(Unknown Source)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:173)\n",
       "\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)\n",
       "\tat com.databricks.spark.util.UsageLogging$$Lambda$8059/871972499.apply(Unknown Source)\n",
       "\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)\n",
       "\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)\n",
       "\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)\n",
       "\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:84)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:378)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$3(StreamExecution.scala:282)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$8057/45218505.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:282)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$8056/1202945115.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:281)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o573.stop.\n: java.util.concurrent.TimeoutException: Stream Execution thread for stream [id = ef5ae52f-dbf6-49b4-929b-9968ae3f0d1a, runId = e7b396bc-19f6-462a-863b-4168c2309ae6] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:716)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:409)\n\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: The stream thread was last executing:\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1265)\n\tat kafkashaded.org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:667)\n\tat kafkashaded.org.apache.kafka.clients.admin.Admin.close(Admin.java:154)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.stopAdmin(KafkaOffsetReaderAdmin.scala:555)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.close(KafkaOffsetReaderAdmin.scala:114)\n\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.stop(KafkaMicroBatchStream.scala:382)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:631)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:629)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9439/1496207081.apply(Unknown Source)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:629)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$7(StreamExecution.scala:525)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9438/1527686176.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:514)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$8058/399919099.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging$$Lambda$466/2118482375.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:173)\n\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging$$Lambda$8059/871972499.apply(Unknown Source)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:84)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:378)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$3(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$8057/45218505.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$8056/1202945115.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:281)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o573.stop.\n: java.util.concurrent.TimeoutException: Stream Execution thread for stream [id = ef5ae52f-dbf6-49b4-929b-9968ae3f0d1a, runId = e7b396bc-19f6-462a-863b-4168c2309ae6] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:716)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:409)\n\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: The stream thread was last executing:\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1265)\n\tat kafkashaded.org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:667)\n\tat kafkashaded.org.apache.kafka.clients.admin.Admin.close(Admin.java:154)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.stopAdmin(KafkaOffsetReaderAdmin.scala:555)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.close(KafkaOffsetReaderAdmin.scala:114)\n\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.stop(KafkaMicroBatchStream.scala:382)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:631)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:629)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9439/1496207081.apply(Unknown Source)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:629)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$7(StreamExecution.scala:525)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9438/1527686176.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:514)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$8058/399919099.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging$$Lambda$466/2118482375.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:173)\n\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging$$Lambda$8059/871972499.apply(Unknown Source)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:84)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:378)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$3(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$8057/45218505.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$8056/1202945115.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:281)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-1719398306228774>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Stop the streaming query\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:372\u001B[0m, in \u001B[0;36mStreamingQuery.stop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstop\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    350\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;124;03m    Stop this streaming query.\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 372\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o573.stop.\n: java.util.concurrent.TimeoutException: Stream Execution thread for stream [id = ef5ae52f-dbf6-49b4-929b-9968ae3f0d1a, runId = e7b396bc-19f6-462a-863b-4168c2309ae6] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:716)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:409)\n\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: The stream thread was last executing:\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1265)\n\tat kafkashaded.org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:667)\n\tat kafkashaded.org.apache.kafka.clients.admin.Admin.close(Admin.java:154)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.stopAdmin(KafkaOffsetReaderAdmin.scala:555)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.close(KafkaOffsetReaderAdmin.scala:114)\n\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.stop(KafkaMicroBatchStream.scala:382)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:631)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:629)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9439/1496207081.apply(Unknown Source)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:629)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$7(StreamExecution.scala:525)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9438/1527686176.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:514)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$8058/399919099.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging$$Lambda$466/2118482375.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:173)\n\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging$$Lambda$8059/871972499.apply(Unknown Source)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:84)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:378)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$3(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$8057/45218505.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$8056/1202945115.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:281)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop the streaming query\n",
    "query.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1e8938-b175-4cf7-aad6-4cf4359e8a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MusicStreamingAnalytics",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}